{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shedai/llm-fine-tuning-dpo?scriptVersionId=256537211\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"107fce93","metadata":{"papermill":{"duration":0.003551,"end_time":"2025-08-18T02:52:24.77462","exception":false,"start_time":"2025-08-18T02:52:24.771069","status":"completed"},"tags":[]},"source":["# RLHF, DPO, LoRA ve Nicemleme (Quantization) – Oyuncak Bir Ortamda Kavramsal Pekiştirme"]},{"cell_type":"markdown","id":"a92c6adc","metadata":{"papermill":{"duration":0.002752,"end_time":"2025-08-18T02:52:24.780767","exception":false,"start_time":"2025-08-18T02:52:24.778015","status":"completed"},"tags":[]},"source":["Bu not defteri, RLHF (PPO ile), DPO (Direct Preference Optimization), PEFT/LoRA ve dinamik INT8 nicemleme kavramlarını ağır bağımlılıklar olmadan hızlıca deneyimleyebilmeniz için hazırlanmıştır. Ekteki örnek kod, gerçek bir LLM yerine tek-adımlı oyuncak bir politika (ToyLM) üzerinde çalışır; böylece:\n","\n","* LoRA/PEFT ile SFT-benzeri uyarlamayı düşük-rank adaptörlerle (az sayıda eğitilebilir parametre) gözlemler,\n","\n","* PPO’nun clipping ve KL ceza (tasma) mekanizmalarının politika güncellemesine etkisini adım adım izler,\n","\n","* DPO ile tercih çiftlerinden doğrudan log-olasılık farkına dayalı kayıp optimizasyonunu görür,\n","\n","* Dinamik INT8 nicemleme ile boyut ve hız kıyaslarını basit bir çok katmanlı algılayıcı (TinyMLP) üzerinde test edersiniz.\n","\n","Amaç: Kavramsal sezgi kazandırmak; üretim kalitesinde hizalama eğitimi yerine, mekanizmaların “ne yaptığını” salt PyTorch ile göstermek."]},{"cell_type":"markdown","id":"e665977f","metadata":{"papermill":{"duration":0.00262,"end_time":"2025-08-18T02:52:24.786629","exception":false,"start_time":"2025-08-18T02:52:24.784009","status":"completed"},"tags":[]},"source":["## Öğrenme Hedefleri\n","\n","* PEFT/LoRA: Düşük-rank adaptörlerin toplam parametreye oranla eğitilebilir parametreleri nasıl azalttığını ve yine de davranışı nasıl değiştirdiğini görmek.\n","\n","* RLHF—PPO: Politika oranı (ratio), clipping, entropi bonusu, değer (critic) ağı, KL cezası gibi bileşenlerin ödül/kararlılık dengesine etkisini kavramak.\n","\n","* DPO: Seçilen (chosen) ve reddedilen (rejected) yanıtların log-olasılık farkı üzerinden doğrudan tercih uyumunun artırılmasını anlamak.\n","\n","* Nicemleme (INT8): FP32 → INT8 dönüşümünün state_dict boyutu ve geçiş süresi üzerindeki etkilerini görmek."]},{"cell_type":"markdown","id":"096965a7","metadata":{"papermill":{"duration":0.00252,"end_time":"2025-08-18T02:52:24.792055","exception":false,"start_time":"2025-08-18T02:52:24.789535","status":"completed"},"tags":[]},"source":["## Kuramsal Arka Plan (Özet)\n","### PEFT/LoRA\n","\n","* Büyük modelleri tam-fine-tune etmek yerine, bir (veya birkaç) katmanda düşük-rank matrislerle öğrenilebilir bir düzeltme (ΔW) yapılır.\n","\n","* Eğitim maliyetini ve bellek kullanımını dramatik biçimde azaltır; transfer ve çoklu görev senaryolarında pratiklik sağlar.\n","\n","### RLHF (PPO ile)\n","\n","* İnsan tercihleri/puanlayıcıları ile üretilen ödüller üzerinden politikayı optimize eder.\n","\n","* PPO clipping: Politika oranının (r_t = π_θ / π_{θ_old}) aşırı uç güncellemelerini kısıtlayarak istikrar sağlar.\n","\n","* KL cezası: Politikanın bir referans politikadan fazla uzaklaşmasını engeller (dil modelinde “dil akıcılığı/dağılım tutarlılığı” koruması).\n","\n","### DPO\n","\n","* Önce ödül modeli eğit, sonra RL yap yerine; tercih çiftleriyle doğrudan log-olasılık farkını (chosen − rejected) optimize eder.\n","\n","* Basitçe: -log σ(β[log π(y_c|x) − log π(y_r|x)]) kaybını minimize eder; β sıcaklık parametresi duyarlılığı kontrol eder.\n","\n","### Nicemleme (Quantization)\n","\n","* Ağırlık ve/veya aktivasyonları daha düşük bit-derinliğe (örn. 8-bit) indirger.\n","\n","* Dinamik nicemleme CPU’da doğrulamada kolay hız kazanımı ve model boyutunda düşüş sağlar; bazı doğruluk kayıpları olabilir."]},{"cell_type":"markdown","id":"10aeb3ca","metadata":{"papermill":{"duration":0.002595,"end_time":"2025-08-18T02:52:24.797394","exception":false,"start_time":"2025-08-18T02:52:24.794799","status":"completed"},"tags":[]},"source":["## Oyuncak Ortam ve Varsayımlar\n","\n","* Tek adımlı politika: Metin yerine, “komut” (prompt) → tek token eylem.\n","\n","* Sözlük: Yardımsever/zararlı/nötr tokenlar.\n","\n","* Ödül: Yardımsever = +1, Zararlı = −1, Nötr = +0.2 (ayarlanabilir).\n","\n","* LoRA yalnızca son doğrusal katmana uygulanır (öğretici sadelik).\n","\n","* PPO: Küçük batch, az epoch; KL cezası doğrudan p dağılımlarından basitçe hesaplanmış sezgisel bir formdadır.\n","\n","* DPO: Küçük bir tercih listesi; amaç doğrudan logπ(chosen) − logπ(rejected) farkını artırmaktır.\n","\n","* INT8 nicemleme: TinyMLP üzerinde FP32 ↔ INT8 boyut ve hız kıyası."]},{"cell_type":"markdown","id":"0afb3abf","metadata":{"papermill":{"duration":0.002828,"end_time":"2025-08-18T02:52:24.803492","exception":false,"start_time":"2025-08-18T02:52:24.800664","status":"completed"},"tags":[]},"source":["## Kod Akışının Yüksek Düzey Özeti\n","\n","* Sözlük ve Prompts: Yardımsever/zararlı/nötr token seti ve komut listesi tanımlanır.\n","\n","* ToyLM (Politika): Embedding -> Linear(head) yapısı; LoRA aktif ise head için düşük-rank adaptör eklenir.\n","\n","* SFT-benzeri eğitim (LoRA ile): Her prompt için hedef token (etiket) üzerinden NLL minimize edilir; eğitilebilir parametre oranı ve ara metrikler yazdırılır.\n","\n","* PPO:\n","\n","    * Referans politika (başlangıçta base kopyası) ile KL hesaplanır.\n","\n","    * Rollout (tek adım): eylem örneklenir, ödül verilir.\n","\n","    * Clipped policy loss, value loss, entropy bonus ile toplam kayıp geriye yayılır.\n","\n","* DPO:\n","\n","    * Tercih çiftleri üzerinden -log σ(β(Δlogπ)) kaybı optimize edilir.\n","\n","    * Eğitim sonrası logπ(chosen) vs logπ(rejected) farkları raporlanır.\n","\n","* INT8 Nicemleme:\n","\n","    * FP32 ve INT8 state_dict boyutu (hem tensör-toplamı hem de serialize edilmiş bytes) ve geçiş süresi kıyaslanır."]},{"cell_type":"markdown","id":"d20ae522","metadata":{"papermill":{"duration":0.002939,"end_time":"2025-08-18T02:52:24.809227","exception":false,"start_time":"2025-08-18T02:52:24.806288","status":"completed"},"tags":[]},"source":["## Yöntemlerin Amaç ve Sinyal Kaynakları\n","| Yöntem      | Amaç                 | Sinyal                             | Tipik Avantaj                              | Tipik Risk                            |\n","| ----------- | -------------------- | ---------------------------------- | ------------------------------------------ | ------------------------------------- |\n","| LoRA (PEFT) | SFT-benzeri uyarlama | Gözetimli hedefler (etiketler)     | Az eğitilebilir parametre, hızlı fine-tune | Yalnızca hedeflenen katman düzeltmesi |\n","| PPO (RLHF)  | Politika iyileştirme | Ödül (insan/heuristik) + KL cezası | Esneklik, ödüle göre davranış              | Kararlılık için hassas ayar gerek     |\n","| DPO         | Tercih uyumu         | Tercih çifti (chosen/rejected)     | Basit doğrudan optimizasyon                | Tercih verisinin kapsayıcılığı        |\n","| INT8        | Boyut/hız düşürme    | —                                  | Daha küçük model, hızlı inference          | Doğruluk düşebilir                    |\n"]},{"cell_type":"markdown","id":"890889d1","metadata":{"papermill":{"duration":0.0027,"end_time":"2025-08-18T02:52:24.81468","exception":false,"start_time":"2025-08-18T02:52:24.81198","status":"completed"},"tags":[]},"source":["## PPO Hiperparametreleri (Oyuncak Ayarlar)\n","| Parametre    | Açıklama                | Örnek Değer |\n","| ------------ | ----------------------- | ----------- |\n","| `clip_range` | Oran clipping eşiği     | 0.2         |\n","| `vf_coef`    | Değer kaybı katsayısı   | 0.5         |\n","| `ent_coef`   | Entropi bonus katsayısı | 0.01        |\n","| `kl_beta`    | KL cezası ağırlığı      | 0.05        |\n","| `epochs`     | PPO döngü sayısı        | 8           |\n","| `batch_size` | Tek-adım rollout batch  | 16          |\n"]},{"cell_type":"markdown","id":"f665519d","metadata":{"papermill":{"duration":0.002462,"end_time":"2025-08-18T02:52:24.819899","exception":false,"start_time":"2025-08-18T02:52:24.817437","status":"completed"},"tags":[]},"source":["## Ödül Fonksiyonu (Oyuncak)\n","| Sınıf       | Token Örnekleri                                              | Ödül |\n","| ----------- | ------------------------------------------------------------ | ---- |\n","| Yardımsever | `evet`, `lütfen`, `teşekkürler`, `yardim`, `özetle`, `tibbi` | +1.0 |\n","| Zararlı     | `hakaret`, `kotu`                                            | −1.0 |\n","| Nötr        | Diğerleri                                                    | +0.2 |\n"]},{"cell_type":"code","execution_count":1,"id":"7a804486","metadata":{"_cell_guid":"270ff591-374c-46b6-bf40-00348bd26381","_uuid":"14ceb837-cded-4909-bd98-bf4421c35664","collapsed":false,"execution":{"iopub.execute_input":"2025-08-18T02:52:24.828103Z","iopub.status.busy":"2025-08-18T02:52:24.827762Z","iopub.status.idle":"2025-08-18T02:52:36.91443Z","shell.execute_reply":"2025-08-18T02:52:36.913305Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":12.093478,"end_time":"2025-08-18T02:52:36.916179","exception":false,"start_time":"2025-08-18T02:52:24.822701","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[Ortam] device = cpu  (cuda=False)\n","\n","=== BÖLÜM A — PEFT/LoRA: SFT benzeri uyarlama (oyuncak) ===\n","[LoRA] Toplam parametre: 899 | Eğitilebilir(param): 536 (59.62%)\n","[A|adım=15] NLL=0.0008 | mini-acc=1.00\n","[A|adım=30] NLL=0.0000 | mini-acc=1.00\n","[A|adım=45] NLL=0.0000 | mini-acc=1.00\n","[A|adım=60] NLL=0.0000 | mini-acc=1.00\n","\n","[A] Eğitim sonrası örnek tahminler:\n","  - 'selam' -> en olası: [('teşekkürler', 1.0), ('kotu', 1.4207732768056758e-09), ('yardim', 6.513560524279427e-11)] | hedef='teşekkürler'\n","  - 'bana yardim et' -> en olası: [('yardim', 1.0), ('tibbi', 2.7555464576778377e-09), ('kotu', 2.0462284644473527e-10)] | hedef='yardim'\n","  - 'tibbi soru' -> en olası: [('tibbi', 1.0), ('özetle', 3.298697981435339e-10), ('kotu', 7.270879731624547e-11)] | hedef='tibbi'\n","  - 'kötü söz soyle' -> en olası: [('hayir', 1.0), ('yardim', 4.888540128256125e-10), ('özetle', 1.724010934012199e-10)] | hedef='hayir'\n","  - 'detayli acikla' -> en olası: [('özetle', 1.0), ('yardim', 8.021999731155915e-11), ('kotu', 3.842232955408864e-12)] | hedef='özetle'\n","  - 'kisa cevap' -> en olası: [('kisa', 0.9999986886978149), ('hayir', 1.2665842632486601e-06), ('evet', 8.428611107669237e-12)] | hedef='kisa'\n","\n","=== BÖLÜM B — RLHF: PPO (oyuncak) ===\n","[B|epoch=01] policy_loss=0.0000 | value_loss=0.6194 | entropy=0.0000 | mean_reward=+0.600 | mean_KL=2.5200 | KL_penalty(avg)=0.1260\n","[B|epoch=02] policy_loss=0.0000 | value_loss=0.4133 | entropy=0.0001 | mean_reward=+0.700 | mean_KL=2.5787 | KL_penalty(avg)=0.1289\n","[B|epoch=03] policy_loss=-0.0000 | value_loss=0.4251 | entropy=0.0004 | mean_reward=+0.650 | mean_KL=2.4377 | KL_penalty(avg)=0.1219\n","[B|epoch=04] policy_loss=0.0000 | value_loss=0.2722 | entropy=0.0008 | mean_reward=+0.750 | mean_KL=2.5410 | KL_penalty(avg)=0.1271\n","[B|epoch=05] policy_loss=-0.0000 | value_loss=0.0745 | entropy=0.0330 | mean_reward=+0.550 | mean_KL=2.5039 | KL_penalty(avg)=0.1252\n","[B|epoch=06] policy_loss=-0.0000 | value_loss=0.0481 | entropy=0.0026 | mean_reward=+0.700 | mean_KL=2.4432 | KL_penalty(avg)=0.1222\n","[B|epoch=07] policy_loss=0.0000 | value_loss=0.0064 | entropy=0.0012 | mean_reward=+0.700 | mean_KL=2.4910 | KL_penalty(avg)=0.1245\n","[B|epoch=08] policy_loss=-0.0000 | value_loss=0.0054 | entropy=0.0007 | mean_reward=+0.650 | mean_KL=2.4398 | KL_penalty(avg)=0.1220\n","\n","[B] PPO sonrası örnek eylem dağılımları:\n","  - 'bana yardim et' -> top3: [('yardim', 1.0), ('tibbi', 0.0), ('hayir', 0.0)]\n","  - 'kisa cevap' -> top3: [('kisa', 1.0), ('hayir', 0.0), ('evet', 0.0)]\n","  - 'detayli acikla' -> top3: [('özetle', 1.0), ('yardim', 0.0), ('kotu', 0.0)]\n","\n","=== BÖLÜM C — DPO (oyuncak) ===\n","[C|epoch=02] dpo_loss=0.0002\n","[C|epoch=04] dpo_loss=0.0000\n","[C|epoch=06] dpo_loss=0.0000\n","[C|epoch=08] dpo_loss=0.0000\n","[C|epoch=10] dpo_loss=0.0000\n","\n","[C] DPO sonrası tercih edilen/edilmeyen log-olasılık farkları:\n","  - 'detayli acikla': logπ(chosen)=+0.000  vs  logπ(rejected)=-43.206  -> Δ=+43.206\n","  - 'kötü söz soyle': logπ(chosen)=+0.000  vs  logπ(rejected)=-46.466  -> Δ=+46.466\n","  - 'selam': logπ(chosen)=+0.000  vs  logπ(rejected)=-41.795  -> Δ=+41.795\n","  - 'bana yardim et': logπ(chosen)=+0.000  vs  logπ(rejected)=-38.598  -> Δ=+38.598\n","  - 'tibbi soru': logπ(chosen)=+0.000  vs  logπ(rejected)=-38.737  -> Δ=+38.737\n","\n","=== BÖLÜM D — Dinamik Quantization (INT8) ===\n","[D] FP32 model boyutu (yaklaşık Tensor-toplamı): 0.019 MB | serialized: 0.021 MB\n","[D] INT8 (dinamik) state_dict boyutu (Tensor-toplamı): 0.000 MB | serialized: 0.008 MB (bazı meta veriler float kalabilir)\n","[D] FP32 geçiş süresi: 1.17 ms | INT8 geçiş süresi: 11.20 ms (CPU)\n","\n","=== BİTTİ ===\n","\n","Öneri: Kodun bölümlerini değiştirerek, ödül fonksiyonunu ve tercih çiftlerini farklılaştırıp etkisini gözlemleyin.\n"]}],"source":["# Kaggle-Runnable Educational Demo: RLHF (PPO), DPO, LoRA (PEFT), and Quantization (Toy Implementations)\n","# ---------------------------------------------------------------------------------------------\n","# Amaç:\n","# - RLHF/PPO, DPO, PEFT/LoRA ve quantization kavramlarını *hızlı* ve *oyuncak* (toy) bir ortamda\n","#   göstermek. Her bölüm, ara çıktılar (print/log) üretir ve adım adım mantığı pekiştirir.\n","# - Bu kod ağır HF/TRL bağımlılıklarına ihtiyaç duymaz; yalnızca PyTorch kullanır.\n","# - Eğitim gerçek bir LLM yerine küçük bir \"oyuncak politika\" (ToyLM) üzerinde gerçekleşir.\n","#\n","# Kullanım:\n","# - Kaggle Notebook'ta tek bir hücreye yapıştırıp çalıştırabilirsiniz.\n","#\n","# İçerik:\n","#   1) Yardımcılar ve Ortam\n","#   2) PEFT/LoRA (oyuncak) ile SFT benzeri uyarlama\n","#   3) RLHF: PPO (oyuncak) – clipping ve KL \"tasması\" mantığı\n","#   4) DPO (oyuncak) – tercih çiftleri ile doğrudan optimizasyon\n","#   5) Dinamik Nicemleme (Quantization) – INT8 örneği ve boyut/performans etkisi\n","#\n","# Not: Bu bir *öğretici* koddur; üretim kalitesinde hizalama eğitimi yerine kavramları\n","#      hızlı simüle eder. Basitlik için tek-adımlık eylemler ve küçük sözlükler kullanılır.\n","# ---------------------------------------------------------------------------------------------\n","\n","import math, random, os, time, io\n","from dataclasses import dataclass\n","from typing import List, Tuple, Dict\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","SEED = 42\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"[Ortam] device = {device}  (cuda={torch.cuda.is_available()})\")\n","\n","# -------------------------\n","# 1) Oyuncak Sözlük / Veri\n","# -------------------------\n","# Basit bir \"komut -> yanıt\" dünyası: bazı yanıtlar 'yardımsever/güvenli', bazıları 'uygunsuz/zararlı' olsun.\n","TOKENS = [\n","    \"evet\", \"hayir\", \"lütfen\", \"teşekkürler\", \"yardim\", \"hakaret\", \"kotu\",\n","    \"tibbi\", \"özetle\", \"uzun\", \"kisa\"\n","]\n","VOCAB = {t:i for i,t in enumerate(TOKENS)}\n","ID2TOK = {i:t for t,i in VOCAB.items()}\n","NUM_ACTIONS = len(TOKENS)\n","\n","HELPFUL = {\"evet\",\"lütfen\",\"teşekkürler\",\"yardim\",\"özetle\",\"tibbi\"}\n","HARMFUL  = {\"hakaret\",\"kotu\"}\n","NEUTRAL  = set(TOKENS) - HELPFUL - HARMFUL\n","\n","# \"Komut\"ları da çok basit tutuyoruz:\n","PROMPTS = [\n","    \"selam\",\n","    \"bana yardim et\",\n","    \"tibbi soru\",\n","    \"kötü söz soyle\",\n","    \"detayli acikla\",\n","    \"kisa cevap\",\n","]\n","\n","# Gözetimli \"hedef\" etiketler (SFT benzeri).\n","# (Amaç: model, bu komutlara yardımsever/uygun karşılıkları daha olası yapsın.)\n","SFT_TARGETS = {\n","    \"selam\": \"teşekkürler\",\n","    \"bana yardim et\": \"yardim\",\n","    \"tibbi soru\": \"tibbi\",\n","    \"kötü söz soyle\": \"hayir\",   # zararlı talebe nazik red\n","    \"detayli acikla\": \"özetle\",  # sembolik\n","    \"kisa cevap\": \"kisa\"\n","}\n","\n","# -----------------------------\n","# 2) Oyuncak Politika (ToyLM)\n","# -----------------------------\n","class ToyLM(nn.Module):\n","    \"\"\"\n","    Çok basit bir 'tek adım' politika:\n","    prompt -> tek bir eylem (token) dağılımı.\n","    \"\"\"\n","    def __init__(self, d_model=32, lora_rank=0, lora_alpha=1.0):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.prompt_embed = nn.Embedding(len(PROMPTS), d_model)\n","        self.head = nn.Linear(d_model, NUM_ACTIONS, bias=True)\n","\n","        # LoRA için düşük-rank adaptörleri (sadece head katmanına)\n","        self.use_lora = lora_rank > 0\n","        if self.use_lora:\n","            # W_new = W + alpha/r * (A @ B)      (A: [out, r], B: [r, in])\n","            self.lora_A = nn.Parameter(torch.zeros(NUM_ACTIONS, lora_rank))\n","            self.lora_B = nn.Parameter(torch.zeros(lora_rank, d_model))\n","            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n","            nn.init.kaiming_uniform_(self.lora_B, a=math.sqrt(5))\n","            self.lora_alpha = lora_alpha\n","            # Base ağırlıkları dondurulabilir (PEFT mantığı)\n","            for p in self.head.parameters():\n","                p.requires_grad = False\n","        else:\n","            self.lora_A = None\n","            self.lora_B = None\n","\n","    def forward(self, prompt_idx: torch.Tensor):\n","        x = self.prompt_embed(prompt_idx)   # [B, d_model]\n","        base_logits = self.head(x)          # [B, NUM_ACTIONS]\n","        if self.use_lora:\n","            # LoRA katkısı\n","            # (alpha / rank) * x @ B^T @ A^T\n","            lora_term = (self.lora_alpha / self.lora_B.shape[0]) * (x @ self.lora_B.t()) @ self.lora_A.t()\n","            logits = base_logits + lora_term\n","        else:\n","            logits = base_logits\n","        return logits\n","\n","    def policy(self, prompt_idx):\n","        logits = self.forward(prompt_idx)\n","        return F.log_softmax(logits, dim=-1), F.softmax(logits, dim=-1)\n","\n","def prompt_to_idx(p: str) -> int:\n","    return PROMPTS.index(p)\n","\n","def token_to_idx(t: str) -> int:\n","    return VOCAB[t]\n","\n","# -------------------------------\n","# 3) Yardımcı: metrik ve yazdırma\n","# -------------------------------\n","def count_trainable_params(model: nn.Module) -> int:\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def count_all_params(model: nn.Module) -> int:\n","    return sum(p.numel() for p in model.parameters())\n","\n","def sample_action(probs):\n","    return torch.multinomial(probs, num_samples=1).squeeze(-1)\n","\n","def kl_divergence(p_log, p, q_log, q):\n","    # KL(p || q) = sum p * (log p - log q)\n","    return torch.sum(p * (p_log - q_log), dim=-1)\n","\n","# -----------------------------------------\n","# BÖLÜM A — PEFT/LoRA ile \"SFT benzeri\" adım\n","# -----------------------------------------\n","print(\"\\n=== BÖLÜM A — PEFT/LoRA: SFT benzeri uyarlama (oyuncak) ===\")\n","base = ToyLM(d_model=32, lora_rank=8, lora_alpha=8.0).to(device)\n","\n","total_params = count_all_params(base)\n","trainable_params = count_trainable_params(base)\n","print(f\"[LoRA] Toplam parametre: {total_params:,} | Eğitilebilir(param): {trainable_params:,} \"\n","      f\"({100*trainable_params/total_params:.2f}%)\")\n","\n","# SFT benzeri hedef: prompt -> 'hedef token' (çeşitli, küçük set)\n","optim = torch.optim.Adam([p for p in base.parameters() if p.requires_grad], lr=5e-2)\n","loss_hist = []\n","\n","for step in range(60):\n","    batch_prompts = random.sample(PROMPTS, k=4)\n","    idx = torch.tensor([prompt_to_idx(p) for p in batch_prompts], device=device)\n","    tgt = torch.tensor([token_to_idx(SFT_TARGETS[p]) for p in batch_prompts], device=device)\n","\n","    logp, probs = base.policy(idx)\n","    loss = F.nll_loss(logp, tgt)\n","    optim.zero_grad()\n","    loss.backward()\n","    optim.step()\n","    loss_hist.append(loss.item())\n","\n","    if (step+1) % 15 == 0:\n","        with torch.no_grad():\n","            acc = (logp.argmax(-1) == tgt).float().mean().item()\n","        print(f\"[A|adım={step+1:02d}] NLL={loss.item():.4f} | mini-acc={acc:.2f}\")\n","\n","# Küçük bir doğrulama: her prompt için en olası aksiyon\n","with torch.no_grad():\n","    print(\"\\n[A] Eğitim sonrası örnek tahminler:\")\n","    for p in PROMPTS:\n","        idx = torch.tensor([prompt_to_idx(p)], device=device)\n","        logp, probs = base.policy(idx)\n","        topk = torch.topk(probs[0], k=3)\n","        top_tokens = [(ID2TOK[i.item()], topk.values[j].item()) for j,i in enumerate(topk.indices)]\n","        print(f\"  - '{p}' -> en olası: {top_tokens} | hedef='{SFT_TARGETS[p]}'\")\n","\n","# ---------------------------------------------------------\n","# BÖLÜM B — RLHF: PPO (oyuncak) + KL \"tasması\" + clipping\n","# ---------------------------------------------------------\n","print(\"\\n=== BÖLÜM B — RLHF: PPO (oyuncak) ===\")\n","\n","@dataclass\n","class PPOCfg:\n","    clip_range: float = 0.2\n","    lr: float = 1e-2\n","    vf_coef: float = 0.5\n","    ent_coef: float = 0.01\n","    kl_beta: float = 0.05\n","    epochs: int = 8\n","    batch_size: int = 16\n","\n","# Basit bir değer ağı (critic)\n","class ValueNet(nn.Module):\n","    def __init__(self, d_model=32):\n","        super().__init__()\n","        self.v = nn.Linear(d_model, 1)\n","    def forward(self, prompt_idx):\n","        with torch.no_grad():\n","            h = base.prompt_embed(prompt_idx)  # base ile aynı gömme alanını kullan\n","        return self.v(h).squeeze(-1)\n","\n","# Referans politika (KL için \"tasmanın\" bağlı olduğu nokta)\n","ref = ToyLM(d_model=32).to(device)\n","ref.load_state_dict(base.state_dict(), strict=False)\n","for p in ref.parameters():\n","    p.requires_grad = False\n","\n","critic = ValueNet(d_model=32).to(device)\n","ppo_params = [p for p in base.parameters() if p.requires_grad] + list(critic.parameters())\n","ppo_optim = torch.optim.Adam(ppo_params, lr=1e-2)\n","cfg = PPOCfg()\n","\n","def reward_fn(action_token: str) -> float:\n","    if action_token in HELPFUL: return 1.0\n","    if action_token in HARMFUL: return -1.0\n","    return 0.2  # nötr küçük ödül\n","\n","def rollout(batch_size: int):\n","    prompts = random.choices(PROMPTS, k=batch_size)\n","    idx = torch.tensor([prompt_to_idx(p) for p in prompts], device=device)\n","    base_logp, base_probs = base.policy(idx)\n","    actions = sample_action(base_probs)\n","    actions_tok = [ID2TOK[a.item()] for a in actions]\n","    rewards = torch.tensor([reward_fn(t) for t in actions_tok], device=device, dtype=torch.float32)\n","    with torch.no_grad():\n","        ref_logp, ref_probs = ref.policy(idx)\n","        kl = kl_divergence(base_logp.exp(), base_logp.exp(), ref_logp, ref_logp.exp())\n","        # Not: formül sadeleştirilmiş; pratikte p, q karışımıyla yapılır. Amaç: sezgi.\n","    return idx, actions, rewards, base_logp, ref_logp, kl\n","\n","def compute_logp_for_actions(logp_all, actions):\n","    # logp(a_t | s_t) değerlerini çek\n","    return logp_all.gather(1, actions.view(-1,1)).squeeze(1)\n","\n","for epoch in range(1, cfg.epochs+1):\n","    idx, actions, rewards, logp_old_all, ref_logp_all, kl_all = rollout(cfg.batch_size)\n","    logp_old = compute_logp_for_actions(logp_old_all, actions).detach()\n","    values_old = critic(idx).detach()\n","    # \"tasmalı\" ödül: KL cezası (negatif) ekleyelim\n","    with torch.no_grad():\n","        kl_penalty = cfg.kl_beta * torch.mean(kl_all)\n","        returns = rewards - cfg.kl_beta * kl_all  # örnek: bireysel kl cezası\n","        advantages = returns - values_old\n","        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n","\n","    # Politika ve değer güncellemesi\n","    ppo_optim.zero_grad()\n","    logp_new_all, probs_new = base.policy(idx)\n","    logp_new = compute_logp_for_actions(logp_new_all, actions)\n","    ratio = torch.exp(logp_new - logp_old)  # r_t\n","\n","    # Clipped policy loss\n","    unclipped = ratio * advantages\n","    clipped = torch.clamp(ratio, 1.0-cfg.clip_range, 1.0+cfg.clip_range) * advantages\n","    policy_loss = -torch.mean(torch.min(unclipped, clipped))\n","\n","    # Değer kaybı (MSE)\n","    values_new = critic(idx)\n","    value_loss = F.mse_loss(values_new, returns)\n","\n","    # Entropy bonus (keşif)\n","    entropy = -torch.mean(torch.sum(probs_new * logp_new_all, dim=-1))\n","\n","    loss = policy_loss + cfg.vf_coef * value_loss - cfg.ent_coef * entropy\n","    loss.backward()\n","    ppo_optim.step()\n","\n","    with torch.no_grad():\n","        mean_r = rewards.mean().item()\n","        mean_kl = kl_all.mean().item()\n","        print(f\"[B|epoch={epoch:02d}] policy_loss={policy_loss.item():.4f} \"\n","              f\"| value_loss={value_loss.item():.4f} | entropy={entropy.item():.4f} \"\n","              f\"| mean_reward={mean_r:+.3f} | mean_KL={mean_kl:.4f} \"\n","              f\"| KL_penalty(avg)={kl_penalty.item():.4f}\")\n","\n","# Sonuç örneği\n","with torch.no_grad():\n","    print(\"\\n[B] PPO sonrası örnek eylem dağılımları:\")\n","    for p in random.sample(PROMPTS, k=3):\n","        idx = torch.tensor([prompt_to_idx(p)], device=device)\n","        _, probs = base.policy(idx)\n","        topk = torch.topk(probs[0], k=3)\n","        top_tokens = [(ID2TOK[i.item()], round(topk.values[j].item(),3)) for j,i in enumerate(topk.indices)]\n","        print(f\"  - '{p}' -> top3: {top_tokens}\")\n","\n","# ---------------------------------------------\n","# BÖLÜM C — DPO (oyuncak): tercih çiftleriyle\n","# ---------------------------------------------\n","print(\"\\n=== BÖLÜM C — DPO (oyuncak) ===\")\n","# DPO kaybı, kabaca:  -log σ(β [log π(y_c|x) - log π(y_r|x)])\n","# Burada β sıcaklık (genelde ~0.1–0.5). Biz 0.3 alalım.\n","\n","beta = 0.3\n","dpo_optim = torch.optim.Adam([p for p in base.parameters() if p.requires_grad], lr=1e-2)\n","\n","# Örnek tercih veri seti: (prompt, chosen, rejected)\n","pairs = [\n","    (\"kötü söz soyle\", \"hayir\", \"hakaret\"),   # nezaket ve zararsızlık\n","    (\"bana yardim et\", \"yardim\", \"kotu\"),     # yardımseverlik\n","    (\"tibbi soru\", \"tibbi\", \"kotu\"),          # görev uygunluğu\n","    (\"detayli acikla\", \"özetle\", \"kotu\"),\n","    (\"selam\", \"teşekkürler\", \"hakaret\"),\n","]\n","\n","def dpo_step():\n","    base.train()\n","    random.shuffle(pairs)\n","    total_loss = 0.0\n","    for (prompt, chosen, rejected) in pairs:\n","        idx = torch.tensor([prompt_to_idx(prompt)], device=device)\n","        logp_all, _ = base.policy(idx)\n","        logp_c = logp_all[0, token_to_idx(chosen)]\n","        logp_r = logp_all[0, token_to_idx(rejected)]\n","        loss = -F.logsigmoid(beta * (logp_c - logp_r))\n","        dpo_optim.zero_grad()\n","        loss.backward()\n","        dpo_optim.step()\n","        total_loss += loss.item()\n","    return total_loss / len(pairs)\n","\n","for e in range(1, 11):\n","    l = dpo_step()\n","    if e % 2 == 0:\n","        print(f\"[C|epoch={e:02d}] dpo_loss={l:.4f}\")\n","\n","with torch.no_grad():\n","    print(\"\\n[C] DPO sonrası tercih edilen/edilmeyen log-olasılık farkları:\")\n","    for (prompt, chosen, rejected) in pairs:\n","        idx = torch.tensor([prompt_to_idx(prompt)], device=device)\n","        logp_all, _ = base.policy(idx)\n","        lc, lr = logp_all[0, token_to_idx(chosen)].item(), logp_all[0, token_to_idx(rejected)].item()\n","        print(f\"  - '{prompt}': logπ(chosen)={lc:+.3f}  vs  logπ(rejected)={lr:+.3f}  -> Δ={lc-lr:+.3f}\")\n","\n","# ------------------------------------------------------------------\n","# BÖLÜM D — Dinamik Quantization (INT8) ve boyut karşılaştırması\n","# ------------------------------------------------------------------\n","print(\"\\n=== BÖLÜM D — Dinamik Quantization (INT8) ===\")\n","\n","class TinyMLP(nn.Module):\n","    def __init__(self, d_in=64, d_h=64, d_out=NUM_ACTIONS):\n","        super().__init__()\n","        self.fc1 = nn.Linear(d_in, d_h)\n","        self.fc2 = nn.Linear(d_h, d_out)\n","    def forward(self, x):\n","        return self.fc2(F.relu(self.fc1(x)))\n","\n","tiny = TinyMLP().eval()\n","\n","def model_nbytes(state_dict):\n","    # Bazı quantized state_dict girdileri Tensor olmayabilir (ör. torch.dtype).\n","    total = 0\n","    for k, v in state_dict.items():\n","        if torch.is_tensor(v):\n","            total += v.numel() * v.element_size()\n","    return total\n","\n","# Alternatif, en güvenlisi: serialize ederek gerçek bayt sayısını ölçmek\n","def serialized_nbytes(state_dict):\n","    buf = io.BytesIO()\n","    torch.save(state_dict, buf)\n","    return buf.tell()\n","\n","fp32_size = model_nbytes(tiny.state_dict())\n","fp32_saved = serialized_nbytes(tiny.state_dict())\n","print(f\"[D] FP32 model boyutu (yaklaşık Tensor-toplamı): {fp32_size/1e6:.3f} MB | serialized: {fp32_saved/1e6:.3f} MB\")\n","\n","# Dinamik quantization (sadece Linear katmanlarda işe yarar; PyTorch 1.x/2.x uyumlu çağrı)\n","try:\n","    from torch.quantization import quantize_dynamic as _qdyn\n","except Exception:\n","    from torch.ao.quantization import quantize_dynamic as _qdyn\n","\n","tiny_q = _qdyn(tiny, {nn.Linear}, dtype=torch.qint8)\n","\n","q_size = model_nbytes(tiny_q.state_dict())\n","q_saved = serialized_nbytes(tiny_q.state_dict())\n","print(f\"[D] INT8 (dinamik) state_dict boyutu (Tensor-toplamı): {q_size/1e6:.3f} MB | serialized: {q_saved/1e6:.3f} MB (bazı meta veriler float kalabilir)\")\n","\n","# Hız kıyaslaması (basit, CPU)\n","x = torch.randn(4096, 64)\n","with torch.inference_mode():\n","    t0 = time.time(); _= tiny(x); t1 = time.time()\n","    _= tiny_q(x); t2 = time.time()\n","print(f\"[D] FP32 geçiş süresi: {(t1-t0)*1000:.2f} ms | INT8 geçiş süresi: {(t2-t1)*1000:.2f} ms (CPU)\")\n","\n","print(\"\\n=== BİTTİ ===\\n\")\n","print(\"Öneri: Kodun bölümlerini değiştirerek, ödül fonksiyonunu ve tercih çiftlerini farklılaştırıp etkisini gözlemleyin.\")\n"]},{"cell_type":"markdown","id":"0ca3e5a1","metadata":{"papermill":{"duration":0.003006,"end_time":"2025-08-18T02:52:36.922614","exception":false,"start_time":"2025-08-18T02:52:36.919608","status":"completed"},"tags":[]},"source":["## Deneyleri Genişletmek İçin Öneriler\n","\n","* Ödül fonksiyonunu değiştirin: Nötr ödülü 0.0 yapın, zarar cezasını −2.0 deneyin; PPO metrikleri nasıl değişiyor?\n","\n","* KL cezasını kapatıp/aşırı artırıp sonuçları karşılaştırın: kl_beta ∈ {0.0, 0.1, 0.5}.\n","\n","* DPO β duyarlılığı: β ∈ {0.1, 0.3, 0.7} için log-olasılık farklarını izleyin.\n","\n","* LoRA rank değiştirme: lora_rank ∈ {4, 8, 16}; eğitilebilir parametre oranı ve yakınsama.\n","\n","* INT8 yerine FP16 (eğer GPU varsa): Boyut ve hız kıyasını genişletin."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":19.148159,"end_time":"2025-08-18T02:52:38.751056","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-18T02:52:19.602897","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}